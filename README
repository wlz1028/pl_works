Fronend requirement:
● When a keyword is submitted, your frontend should return a list of URLs, or a page indicating results for such keyword cannot be found.
    * check
● The returned list of URLs should be sorted by the PageRank score of each URL.
    * check
● Static pagination or dynamic page loading with AJAX should be used to limit the number of URLs returned by each request sent to the server.
    * static pagination/10 url per page
● Error page should be returned when user is trying to access a page that does not existed, or using a HTTP method that is not supported. The error page should provide a link for user to visit the valid query page.
    * check
    * e.g. search "trythis"
* Frontend source code
  frontEnd/frontEnd.py

Beckend requiremetn:
Compute the PageRank score for each page that is visited by the crawler, given a list of URLs specified in "urls.txt".
    * pageranck score is in mongodb
● Generate and store required data, i.e. lexicon, document index, inverted index, PageRank scores, in persistent storage.
    * data is in databse online

Deliverable
* Backend
  source code crawler: backEnd/crawler.py
* Unit test: 'python backEnd/unit_test.py'
  * please see unit_test.py comment for details
  * single thread cralwer test
  * multi thread cralwer test
  * persistent test

AWS Deployment
● Search engine is active online for one week after the due date of this lab.
  * done
● Public DNS of the server should be included in a README file.
  NOTE: please contact wlz1028@gmail.com if website is offline
  * Public DNS
      ec2-54-86-113-124.compute-1.amazonaws.com
      http://ec2-54-86-113-124.compute-1.amazonaws.com:8080/search
    Public IP
      54.86.113.124
● Data generated by backend should have been already stored in persistent storage at the time when frontend is being deployed on AWS.
  * done (mongodb)
● Persistent storage should contain data crawled from www.eecg.toronto.edu with depth of one.
  * crawl on Nov-21
● Benchmark setup and results in the README file.
        lizwang-Air:lab3 lizwang$ ab -c 700 -n 700 http://54.86.113.124:8080/result/engineering/1
        This is ApacheBench, Version 2.3 <$Revision: 1554214 $>
        Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
        Licensed to The Apache Software Foundation, http://www.apache.org/
        
        Benchmarking 54.86.113.124 (be patient)
        Completed 100 requests
        Completed 200 requests
        Completed 300 requests
        Completed 400 requests
        Completed 500 requests
        Completed 600 requests
        Completed 700 requests
        Finished 700 requests
        
        
        Server Software:        TornadoServer/4.0.2
        Server Hostname:        54.86.113.124
        Server Port:            8080
        
        Document Path:          /result/engineering/1
        Document Length:        2375 bytes
        
        Concurrency Level:      700
        Time taken for tests:   16.275 seconds
        Complete requests:      700
        Failed requests:        0
        Total transferred:      1817900 bytes
        HTML transferred:       1662500 bytes
        Requests per second:    43.01 [#/sec] (mean)
        Time per request:       16274.936 [ms] (mean)
        Time per request:       23.250 [ms] (mean, across all concurrent requests)
        Transfer rate:          109.08 [Kbytes/sec] received
        
        Connection Times (ms)
                      min  mean[+/-sd] median   max
        Connect:       43 1258 1522.8    209    4771
        Processing:   171 2594 1724.0   2409   13890
        Waiting:      171 2593 1723.6   2408   13890
        Total:        231 3852 2635.1   3141   15128
        
        Percentage of the requests served within a certain time (ms)
          50%   3141
          66%   4378
          75%   5748
          80%   6730
          90%   7791
          95%   8458
          98%   9181
          99%  10597
         100%  15128 (longest request)

  ● Maximum number of connections that can be handled by the server before any connection drops.
    * 700
    * Note: In frontEnd.py last line, I use tonado backend instead of auto. It's more stable in terms of handling multithreading
  ● Maximum number of requests per second (RPS) that can be sustained by the server when operating with maximum number of connections.
    * Requests per second:    43.01 [#/sec] (mean)
  ● Average and 99 percentile of response time or latency per request
    * 10597/ (0.99 * 700) = 15.2 #/sec
  ● Utilization of CPU, memory, disk IO, and network when max performance is sustained
    CPU:
        Max CPU usage in 10 trials = 100%
        Command:
            mpstat 1
    Memory:
        Measured by free memory
        Max memory useage = 96656 - 89772 = 6884kb
        Command:
            vmstat 1
    Disk IO:
        Max disk write in 10 trials = 1840.78
        Max disk read in 10 trials = 0 KB/sec
        Command:
            iostat -dkx 1
    Network
        Max receive/sec in 10 trials = 74kb
        Max send/sec 10 trials = 257k
        Command:
            dstat 1

In this lab, since we introduced databse, so each request uses more resource than the lab2(each query request uses database). This reason cuased less connection, more memory useage(due to database), more CPU usage(due to database), more I/O. Lab3 has simmialr rec(kb)/sec to lab2, but more send(kb)/sec, becase lab3 we send a list result urls, and lab2 we used a test page which returned only one word.

Bonus Work
● To claim bonus marks, you must explicitly indicate the bonus features that you have implemented whether it is completed or incomplete, and include it in a README file.
  1) Successfuly impletemt noSQL database(mongodb)
    Improvement, instead of using 4 tables from handout, we created 2 collections(tables) in mongodb. Futhermore, we plan to calculate pagerank by using mongodb mapreduce framework. In long term, mongodb doesn't have schema, so we can introduce more attributes to database.
  2) Multi-thread cralwer: backEnd/crawler_multi_Thread.py
    Create only one cralwer instance. In crawl(), create a thread for each url handling. A thread helper class starts thread automatically, and a helper thread method handles each url. If more than 4 thread(my labptop CPU support 4 thread) is active, we wait. This implementation is fater then timmer.
    Please see comment in crawler_multi_Thread.py for details
    Max number of thread:

    * Robustness testing:
      * Command:
          python crawler_multi_Thread.py
      * Setting
          URL = http://www.eecg.toronto.edu 
          depth =2
      * Result:
        CPU 68% usage
        10 runs without crash
    * Correctness testing:
      * Command:
          python unit_test.py
      * Setting:
          compare number of word_id and url_id again single thread
      * Result:
          Some test runs failed due to different timeout in single thread and multithread. Otherwise, result is the same.
